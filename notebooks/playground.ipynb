{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sieunpark/Documents/GitHub/simple-tokenization\n",
      "/Users/sieunpark/Documents/GitHub/simple-tokenization\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordPieceVocab(token_to_ids={'a': 0, 'b': 1, 'c': 2, 'aaa': 3, 'abc': 4})\n",
      "WordPieceVocab(token_to_ids={'a': 0, 'b': 1, 'c': 2, 'aaa': 3, 'abc': 4, 'EOS': 5, 'BOS': 6, 'SEP': 7})\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy, copy\n",
    "from typing import Union, Optional\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class WordPieceVocab:\n",
    "    token_to_ids: dict = field(default_factory=lambda: {})\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, filepath):\n",
    "        token_to_ids = dict()\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as reader:\n",
    "            tokens = reader.readlines()\n",
    "        for index, token in enumerate(tokens):\n",
    "            token = token.rstrip(\"\\n\")\n",
    "            token_to_ids[token] = index\n",
    "        return cls(token_to_ids=token_to_ids) \n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, token_to_ids: dict):\n",
    "        return cls(token_to_ids=token_to_ids)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.token_to_ids)\n",
    "\n",
    "def add_vocab(vocab, tokens: Union[str, list[str]]):\n",
    "    if isinstance(tokens, str):\n",
    "        tokens = [tokens]\n",
    "\n",
    "    new_token_to_ids = copy(vocab.token_to_ids)\n",
    "    for token in tokens:\n",
    "        new_token_to_ids[token] = len(new_token_to_ids)        \n",
    "\n",
    "    return WordPieceVocab.from_dict(new_token_to_ids)\n",
    "\n",
    "def add_special_tokens(\n",
    "    vocab,\n",
    "    )\n",
    "    \n",
    "vocab = WordPieceVocab.from_file(\"sample-vocab.txt\")\n",
    "print(vocab)\n",
    "\n",
    "vocab = add_vocab(vocab, tokens=[\"EOS\", \"BOS\", \"SEP\"])\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normailzers\n",
    "\n",
    "def unicode_normalize(text, format: str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordPieceVocab(vocab={'a': 0, 'b': 1, 'c': 2, 'aaa': 3, 'abc': 4, 'j': 10})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class WordPieceConfig:\n",
    "    lowercase: bool = \n",
    "\n",
    "def wordpiece_tokenize(text: str, vocab: WordPieceVocab, config: WordPieceConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid character removal and whitespace cleanup on text\n",
    "\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File name too long (os error 63)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mokenization is the very first step in most text processing works. Not surprisingly, tremendous academic efforts have been made to find the best tokenization method for various NLP tasks. For the past few years, Byte Pair Encoding (BPE) (Gage, 1994) has been considered the de facto standard tokenization technique since it was reintroduced by Sennrich et al. (2016a). Besides the fact that BPE turns out to be very effective in the machine translation task, another important reason BPE has gained âˆ—*Equal contribution. such popularity is that BPE is a data-driven statistical algorithm so it is independent of language. However, it is still not clear whether BPE works best across all languages, irrespective of tasks. In this paper we study various tokenization strategies for Korean, a language which is morphologically by far richer than English. Concretely, we empirically examine what is the best tokenization strategy for Korean to English / English to Korean machine translation tasks, and natural language understanding (NLU) tasksâ€”machine reading comprehension (MRC), natural language inference (NLI), semantic textual similarity (STS), sentiment analysis, and paraphrase identification. We are particularly interested in how complementary BPE and linguistically motivated segmentation are. 2 Background 2.1 MeCab-ko: A Korean Morphological Analyzer MeCab (Kudo, 2006) is an open-source morphological analyzer based on Conditional Random Fields (CRFs). It is originally designed for Japanese, but also serves generic purposes so it can be applied to other languages. MeCab-ko1, a Korean extension of MeCab, started from the idea that MeCab can be easily extended to the Korean language due to the close similarity between Japanese and Korean in terms of morphology or syntax. MeCab-ko trained its model on the Sejong Corpus (Kang and Kim, 2001), arguably the largest Korean corpus morphologically annotated by many experts, using MeCab. Ever since released in 2013, MeCab-ko has been widely used for many Korean NLP tasks due to its high accuracy and good usability. For example, the Workshop on Asian Transla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m tk \u001b[38;5;241m=\u001b[39m SentencePieceBPETokenizer()\n\u001b[0;32m----> 4\u001b[0m tk\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m      5\u001b[0m     files\u001b[38;5;241m=\u001b[39ms,\n\u001b[1;32m      6\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m tk\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tokenizers/implementations/sentencepiece_bpe.py:75\u001b[0m, in \u001b[0;36mSentencePieceBPETokenizer.train\u001b[0;34m(self, files, vocab_size, min_frequency, special_tokens, limit_alphabet, initial_alphabet, show_progress)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(files, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     74\u001b[0m     files \u001b[38;5;241m=\u001b[39m [files]\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mtrain(files, trainer\u001b[38;5;241m=\u001b[39mtrainer)\n",
      "\u001b[0;31mException\u001b[0m: File name too long (os error 63)"
     ]
    }
   ],
   "source": [
    "tk = SentencePieceBPETokenizer()\n",
    "\n",
    "tk.train(\n",
    "    files=s,\n",
    "    vocab_size=10,\n",
    ")\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tk = AutoTokenizer.from_pretrained(\"baseten/Meta-Llama-3-tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 9906, 11, 856, 836, 374, 445, 81101, 13]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = tk.encode(\"Hello, my name is Llama.\")\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>Hello, my name is Llama.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = tk.decode(enc)\n",
    "dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' my'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.decode([856])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([9906, 11, 856, 836, 374, 445, 81101, 13],\n",
       " ['Hello,', 'my', 'name', 'is', 'Llama.'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello, my name is Llama.\"\n",
    "tokens = tk.encode(text, add_special_tokens=False)\n",
    "decoded_tokens = tk.decode(tokens).split()\n",
    "\n",
    "tokens, decoded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_my'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.decode(856).replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoded_tokens \u001b[38;5;241m=\u001b[39m tk\u001b[38;5;241m.\u001b[39mdecode([[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3754\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3751\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3752\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3755\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3756\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3757\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3758\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3759\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:593\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    592\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 593\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens)\n\u001b[1;32m    595\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    596\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "decoded_tokens = tk.decode([[t] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9906], [11], [856], [836], [374], [445], [81101], [13]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[t] for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tk\u001b[38;5;241m.\u001b[39mdecode([[\u001b[38;5;241m9906\u001b[39m], [\u001b[38;5;241m11\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3754\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3751\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3752\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[1;32m   3755\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[1;32m   3756\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3757\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3758\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3759\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:593\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    592\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 593\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mdecode(token_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens)\n\u001b[1;32m    595\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    596\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    599\u001b[0m )\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 9468, 248, 222]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.encode(\"ðŸš€\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ï¿½'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.decode(9468)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tk# Token ID\n",
    "token_id = 9468\n",
    "\n",
    "# Decode the token ID to a string\n",
    "decoded_string = tokenizer.decode([token_id])\n",
    "\n",
    "# Convert the decoded string to bytes\n",
    "decoded_bytes = decoded_string.encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded string: ï¿½\n",
      "Decoded bytes: b'\\xef\\xbf\\xbd'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decoded string: {decoded_string}\")\n",
    "print(f\"Decoded bytes: {decoded_bytes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer class name: PreTrainedTokenizerFast\n",
      "Parent classes: ['PreTrainedTokenizerBase']\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "\n",
    "# Get the class name of the tokenizer\n",
    "print(\"Tokenizer class name:\", tokenizer.__class__.__name__)\n",
    "\n",
    "# Get the parent (base) classes of the tokenizer class\n",
    "parent_classes = [base.__name__ for base in tokenizer.__class__.__bases__]\n",
    "print(\"Parent classes:\", parent_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final BPE Vocabulary:\n",
      "Initial Vocabulary:\n",
      "t h i s: 1\n",
      "i s: 2\n",
      "a: 1\n",
      "t e s t .: 1\n",
      "w e: 1\n",
      "a r e: 1\n",
      "t e s t i n g: 1\n",
      "B P E: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "r e a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 1 ---\n",
      "Most frequent pair: ('i', 's')\n",
      "Pair frequency: 3\n",
      "Updated Vocabulary:\n",
      "t h is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "t e s t .: 1\n",
      "w e: 1\n",
      "a r e: 1\n",
      "t e s t i n g: 1\n",
      "B P E: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "r e a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 2 ---\n",
      "Most frequent pair: ('t', 'e')\n",
      "Pair frequency: 2\n",
      "Updated Vocabulary:\n",
      "t h is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "te s t .: 1\n",
      "w e: 1\n",
      "a r e: 1\n",
      "te s t i n g: 1\n",
      "B P E: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "r e a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 3 ---\n",
      "Most frequent pair: ('te', 's')\n",
      "Pair frequency: 2\n",
      "Updated Vocabulary:\n",
      "t h is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "tes t .: 1\n",
      "w e: 1\n",
      "a r e: 1\n",
      "tes t i n g: 1\n",
      "B P E: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "r e a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 4 ---\n",
      "Most frequent pair: ('tes', 't')\n",
      "Pair frequency: 2\n",
      "Updated Vocabulary:\n",
      "t h is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "test .: 1\n",
      "w e: 1\n",
      "a r e: 1\n",
      "test i n g: 1\n",
      "B P E: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "r e a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 5 ---\n",
      "Most frequent pair: ('r', 'e')\n",
      "Pair frequency: 2\n",
      "Updated Vocabulary:\n",
      "t h is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "test .: 1\n",
      "w e: 1\n",
      "a re: 1\n",
      "test i n g: 1\n",
      "B P E: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "re a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 6 ---\n",
      "Most frequent pair: ('B', 'P')\n",
      "Pair frequency: 2\n",
      "Updated Vocabulary:\n",
      "t h is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "test .: 1\n",
      "w e: 1\n",
      "a re: 1\n",
      "test i n g: 1\n",
      "BP E: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "re a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 7 ---\n",
      "Most frequent pair: ('BP', 'E')\n",
      "Pair frequency: 2\n",
      "Updated Vocabulary:\n",
      "t h is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "test .: 1\n",
      "w e: 1\n",
      "a re: 1\n",
      "test i n g: 1\n",
      "BPE: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "re a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 8 ---\n",
      "Most frequent pair: ('t', 'h')\n",
      "Pair frequency: 1\n",
      "Updated Vocabulary:\n",
      "th is: 1\n",
      "is: 2\n",
      "a: 1\n",
      "test .: 1\n",
      "w e: 1\n",
      "a re: 1\n",
      "test i n g: 1\n",
      "BPE: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "re a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 9 ---\n",
      "Most frequent pair: ('th', 'is')\n",
      "Pair frequency: 1\n",
      "Updated Vocabulary:\n",
      "this: 1\n",
      "is: 2\n",
      "a: 1\n",
      "test .: 1\n",
      "w e: 1\n",
      "a re: 1\n",
      "test i n g: 1\n",
      "BPE: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "re a l l y: 1\n",
      "c o o l .: 1\n",
      "\n",
      "--- Merge Operation 10 ---\n",
      "Most frequent pair: ('test', '.')\n",
      "Pair frequency: 1\n",
      "Updated Vocabulary:\n",
      "this: 1\n",
      "is: 2\n",
      "a: 1\n",
      "test.: 1\n",
      "w e: 1\n",
      "a re: 1\n",
      "test i n g: 1\n",
      "BPE: 2\n",
      "t o k e n i z a t i o n .: 1\n",
      "re a l l y: 1\n",
      "c o o l .: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "def get_vocab(text):\n",
    "    words = text.split()\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for word in words:\n",
    "        vocab[' '.join(word)] += 1\n",
    "    return vocab\n",
    "\n",
    "def bpe_tokenization(text, num_merges):\n",
    "    vocab = get_vocab(text)\n",
    "    print(\"Initial Vocabulary:\")\n",
    "    for word, freq in vocab.items():\n",
    "        print(f\"{word}: {freq}\")\n",
    "    \n",
    "    for i in range(num_merges):\n",
    "        print(f\"\\n--- Merge Operation {i+1} ---\")\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            print(\"No more pairs to merge.\")\n",
    "            break\n",
    "        \n",
    "        best = max(pairs, key=pairs.get)\n",
    "        print(f\"Most frequent pair: {best}\")\n",
    "        print(f\"Pair frequency: {pairs[best]}\")\n",
    "        \n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        print(\"Updated Vocabulary:\")\n",
    "        for word, freq in vocab.items():\n",
    "            print(f\"{word}: {freq}\")\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "text = \"this is a test. we are testing BPE tokenization. BPE is really cool.\"\n",
    "num_merges = 10\n",
    "\n",
    "print(\"Final BPE Vocabulary:\")\n",
    "final_vocab = bpe_tokenization(text, num_merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom BPE Tokens:\n",
      "['B', 'P', 'E']\n",
      "['tokeni', 'z', 'at', 'i', 'o', 'n']\n",
      "['i', 's']\n",
      "['a', 'w', 'e', 's', 'o', 'm', 'e']\n",
      "['f', 'or']\n",
      "['n', 'at', 'u', 'r', 'a', 'l']\n",
      "['l', 'an', 'g', 'u', 'a', 'g', 'e']\n",
      "['p', 'r', 'o', 'c', 'e', 's', 's', 'in', 'g', '.']\n",
      "\n",
      "HuggingFace Tokens:\n",
      "['B', 'PE', 'Ä token', 'ization', 'Ä is', 'Ä awesome', 'Ä for', 'Ä natural', 'Ä language', 'Ä processing', '.']\n",
      "\n",
      "Comparison:\n",
      "Custom BPE unique tokens: 25\n",
      "HuggingFace unique tokens: 11\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import collections\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, text, num_merges):\n",
    "        \"\"\"\n",
    "        Initialize BPE Tokenizer\n",
    "        \n",
    "        Args:\n",
    "            text (str): Training text for initial vocabulary\n",
    "            num_merges (int): Number of merge operations\n",
    "        \"\"\"\n",
    "        self.num_merges = num_merges\n",
    "        self.vocab = self.get_vocab(text)\n",
    "        self.merge_rules = []\n",
    "        self.perform_bpe()\n",
    "\n",
    "    def get_vocab(self, text):\n",
    "        \"\"\"Create initial vocabulary of characters\"\"\"\n",
    "        words = text.split()\n",
    "        vocab = collections.defaultdict(int)\n",
    "        for word in words:\n",
    "            vocab[' '.join(word)] += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_stats(self):\n",
    "        \"\"\"Calculate pair frequencies\"\"\"\n",
    "        pairs = collections.defaultdict(int)\n",
    "        for word, freq in self.vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "\n",
    "    def merge_vocab(self, pair):\n",
    "        \"\"\"Merge most frequent pair in vocabulary\"\"\"\n",
    "        new_vocab = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        \n",
    "        for word in self.vocab:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            new_vocab[w_out] = self.vocab[word]\n",
    "        \n",
    "        return new_vocab\n",
    "\n",
    "    def perform_bpe(self):\n",
    "        \"\"\"Perform Byte Pair Encoding\"\"\"\n",
    "        for _ in range(self.num_merges):\n",
    "            pairs = self.get_stats()\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            best = max(pairs, key=pairs.get)\n",
    "            self.merge_rules.append(best)\n",
    "            self.vocab = self.merge_vocab(best)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Tokenize new text using learned merge rules\"\"\"\n",
    "        words = text.split()\n",
    "        tokenized_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Start with characters\n",
    "            chars = ' '.join(word)\n",
    "            \n",
    "            # Apply merge rules\n",
    "            for merge in self.merge_rules:\n",
    "                bigram = ' '.join(merge)\n",
    "                chars = chars.replace(bigram, ''.join(merge))\n",
    "            \n",
    "            tokenized_words.append(chars.split())\n",
    "        \n",
    "        return tokenized_words\n",
    "\n",
    "def verify_with_huggingface(bpe_tokens, text):\n",
    "    \"\"\"\n",
    "    Verify BPE tokenization against HuggingFace GPT-2 Tokenizer\n",
    "    \n",
    "    Args:\n",
    "        bpe_tokens (list): Tokens from custom BPE tokenizer\n",
    "        text (str): Original text\n",
    "    \"\"\"\n",
    "    # Use GPT-2 tokenizer as reference\n",
    "    hf_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    # Tokenize with HuggingFace\n",
    "    hf_tokens = hf_tokenizer.tokenize(text)\n",
    "    \n",
    "    print(\"Custom BPE Tokens:\")\n",
    "    for word_tokens in bpe_tokens:\n",
    "        print(word_tokens)\n",
    "    \n",
    "    print(\"\\nHuggingFace Tokens:\")\n",
    "    print(hf_tokens)\n",
    "    \n",
    "    # Basic comparison\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"Custom BPE unique tokens: {len(set(sum(bpe_tokens, [])))}\") \n",
    "    print(f\"HuggingFace unique tokens: {len(set(hf_tokens))}\")\n",
    "\n",
    "# Example usage\n",
    "training_text = \"If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in Chapter 2, we saw that most Transformer models use a subword tokenization algorithm. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call training. The exact rules that govern this training depend on the type of tokenizer used, and weâ€™ll go over the three main algorithms later in this chapter.\"\n",
    "inference_text = \"BPE tokenization is awesome for natural language processing.\"\n",
    "\n",
    "# Create BPE tokenizer\n",
    "bpe_tokenizer = BPETokenizer(training_text, num_merges=20)\n",
    "\n",
    "# Tokenize inference text\n",
    "bpe_tokens = bpe_tokenizer.tokenize(inference_text)\n",
    "\n",
    "# Verify with HuggingFace\n",
    "verify_with_huggingface(bpe_tokens, inference_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in Chapter 2, we saw that most Transformer models use a subword tokenization algorithm. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call training. The exact rules that govern this training depend on the type of tokenizer used, and weâ€™ll go over the three main algorithms later in this chapter.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_tokenizer = BPETokenizer(training_text, num_merges=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'n'),\n",
       " ('t', 'h'),\n",
       " ('e', 'r'),\n",
       " ('a', 't'),\n",
       " ('e', 'n'),\n",
       " ('a', 'n'),\n",
       " ('th', 'e'),\n",
       " ('o', 'r'),\n",
       " ('t', 'o'),\n",
       " ('m', 'o'),\n",
       " ('r', 'e'),\n",
       " ('y', 'o'),\n",
       " ('yo', 'u'),\n",
       " ('s', 't'),\n",
       " ('e', 'd'),\n",
       " ('u', 's'),\n",
       " ('a', 'in'),\n",
       " ('to', 'k'),\n",
       " ('tok', 'en'),\n",
       " ('token', 'i')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.merge_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 179\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[57], line 166\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Test encoding and decoding\u001b[39;00m\n\u001b[1;32m    165\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world of NLP!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 166\u001b[0m encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(test_text)\n\u001b[1;32m    167\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(encoded)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_text)\n",
      "Cell \u001b[0;32mIn[57], line 123\u001b[0m, in \u001b[0;36mBPETokenizer.encode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Find the first pair that exists in merges\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m mergeable_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((pair \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m pairs \u001b[38;5;28;01mif\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerges), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# If no mergeable pair found, break\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mergeable_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Initialize BPE Tokenizer\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Maximum number of tokens in the vocabulary\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.merges = {}\n",
    "        self.token_freqs = None\n",
    "    \n",
    "    def _get_stats(self, tokens: List[str]) -> Counter:\n",
    "        \"\"\"\n",
    "        Count pair frequencies in the tokens\n",
    "        \n",
    "        Args:\n",
    "            tokens (List[str]): List of tokens\n",
    "        \n",
    "        Returns:\n",
    "            Counter of token pair frequencies\n",
    "        \"\"\"\n",
    "        pairs = Counter()\n",
    "        for token in tokens:\n",
    "            symbols = token.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[tuple(symbols[i:i+2])] += 1\n",
    "        return pairs\n",
    "    \n",
    "    def _merge_tokens(self, tokens: List[str], pair: Tuple[str, str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Merge the most frequent pair of tokens\n",
    "        \n",
    "        Args:\n",
    "            tokens (List[str]): List of tokens\n",
    "            pair (Tuple[str, str]): Pair to merge\n",
    "        \n",
    "        Returns:\n",
    "            List of merged tokens\n",
    "        \"\"\"\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            # Find indices where the pair occurs\n",
    "            try:\n",
    "                j = tokens.index(pair[0], i)\n",
    "                if j + 1 < len(tokens) and tokens[j + 1] == pair[1]:\n",
    "                    # Merge the pair\n",
    "                    new_tokens.append(pair[0] + pair[1])\n",
    "                    i = j + 2\n",
    "                else:\n",
    "                    new_tokens.append(tokens[i])\n",
    "                    i += 1\n",
    "            except ValueError:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def train(self, corpus: List[str]):\n",
    "        \"\"\"\n",
    "        Train BPE tokenizer on the given corpus\n",
    "        \n",
    "        Args:\n",
    "            corpus (List[str]): List of training texts\n",
    "        \"\"\"\n",
    "        # Preprocess and initialize tokens\n",
    "        tokens = [' '.join(list(text)) for text in corpus]\n",
    "        \n",
    "        # Initialize token frequencies\n",
    "        self.token_freqs = Counter(' '.join(tokens).split())\n",
    "        \n",
    "        # Merge until reaching vocab size\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            # Get pair frequencies\n",
    "            pairs = self._get_stats(tokens)\n",
    "            \n",
    "            # If no more pairs can be merged, break\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Get the most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            \n",
    "            # Merge tokens\n",
    "            tokens = [' '.join(self._merge_tokens(token.split(), best_pair)) \n",
    "                      for token in tokens]\n",
    "            \n",
    "            # Update vocabulary and merges\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab[merged_token] = len(self.vocab)\n",
    "            self.merges[best_pair] = merged_token\n",
    "        \n",
    "        # Create reverse vocabulary\n",
    "        self.id_to_token = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encode text into token ids\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to encode\n",
    "        \n",
    "        Returns:\n",
    "            List of token ids\n",
    "        \"\"\"\n",
    "        # Convert to list of characters\n",
    "        tokens = list(text)\n",
    "        \n",
    "        # Apply merge rules\n",
    "        while len(tokens) > 1:\n",
    "            pairs = self._get_stats([' '.join(tokens)])\n",
    "            \n",
    "            # If no mergeable pairs, break\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Find the first pair that exists in merges\n",
    "            mergeable_pair = next((pair for pair in pairs if pair in self.merges), None)\n",
    "            \n",
    "            # If no mergeable pair found, break\n",
    "            if mergeable_pair is None:\n",
    "                break\n",
    "            \n",
    "            # Merge the pair\n",
    "            tokens = self._merge_tokens(tokens, mergeable_pair)\n",
    "        \n",
    "        # Convert to token ids\n",
    "        return [self.vocab.get(token, self.vocab.get('<unk>', 0)) for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode token ids back to text\n",
    "        \n",
    "        Args:\n",
    "            token_ids (List[int]): List of token ids to decode\n",
    "        \n",
    "        Returns:\n",
    "            Decoded text\n",
    "        \"\"\"\n",
    "        # Convert ids to tokens\n",
    "        tokens = [self.id_to_token.get(tid, '<unk>') for tid in token_ids]\n",
    "        \n",
    "        # Join tokens\n",
    "        return ''.join(tokens)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Training corpus\n",
    "    corpus = [\n",
    "        \"Hello world!\",\n",
    "        \"Natural language processing is fascinating.\",\n",
    "        \"Tokenization breaks text into meaningful pieces.\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize and train tokenizer\n",
    "    tokenizer = BPETokenizer(vocab_size=100)\n",
    "    tokenizer.train(corpus)\n",
    "    \n",
    "    # Test encoding and decoding\n",
    "    test_text = \"Hello world of NLP!\"\n",
    "    encoded = tokenizer.encode(test_text)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    \n",
    "    print(\"Original Text:\", test_text)\n",
    "    print(\"Encoded Tokens:\", encoded)\n",
    "    print(\"Decoded Text:\", decoded)\n",
    "    \n",
    "    # Print vocabulary\n",
    "    print(\"\\nVocabulary:\")\n",
    "    for token, token_id in list(tokenizer.vocab.items())[:10]:\n",
    "        print(f\"{token}: {token_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in Chapter 2, we saw that most Transformer models use a subword tokenization algorithm. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call training. The exact rules that govern this training depend on the type of tokenizer used, and weâ€™ll go over the three main algorithms later in this chapter.\"\n",
    "vocab_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text into words and count frequencies\n",
    "words = text.split()\n",
    "word_freq = defaultdict(int)\n",
    "for word in words:\n",
    "    chars = list(word) + ['</w>']\n",
    "    word_freq[tuple(chars)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge Rules: [(('e', '</w>'), 'e</w>'), (('t', '</w>'), 't</w>'), (('i', 'n'), 'in'), (('t', 'h'), 'th'), (('s', '</w>'), 's</w>'), (('e', 'r'), 'er'), (('e', 'n'), 'en'), (('th', 'e</w>'), 'the</w>'), (('t', 'o'), 'to'), (('o', 'r'), 'or'), (('d', '</w>'), 'd</w>'), (('a', 'n'), 'an'), (('l', '</w>'), 'l</w>'), (('er', '</w>'), 'er</w>'), (('a', 't</w>'), 'at</w>'), (('m', 'o'), 'mo'), (('in', '</w>'), 'in</w>'), (('y', 'o'), 'yo'), (('yo', 'u'), 'you'), (('to', 'k'), 'tok'), (('tok', 'en'), 'token'), (('token', 'i'), 'tokeni'), (('tokeni', 'z'), 'tokeniz'), (('r', 'a'), 'ra'), (('a', '</w>'), 'a</w>'), (('y', '</w>'), 'y</w>'), (('t', 'ra'), 'tra'), (('s', 't</w>'), 'st</w>'), (('r', '</w>'), 'r</w>'), (('o', 'n'), 'on'), (('l', 'l</w>'), 'll</w>'), (('d', 'e'), 'de'), (('.', '</w>'), '.</w>'), ((',', '</w>'), ',</w>'), (('you', 'r</w>'), 'your</w>'), (('tra', 'in'), 'train'), (('tokeniz', 'er</w>'), 'tokenizer</w>'), (('r', 'e'), 're'), (('mo', 'de'), 'mode'), (('in', 'g'), 'ing'), (('i', 's</w>'), 'is</w>'), (('f', '</w>'), 'f</w>'), (('e', 'd</w>'), 'ed</w>'), (('a', 't'), 'at'), (('w', 'e</w>'), 'we</w>'), (('u', 's</w>'), 'us</w>'), (('u', 's'), 'us'), (('u', 'a'), 'ua'), (('ua', 'g'), 'uag'), (('uag', 'e</w>'), 'uage</w>'), (('train', 'ing'), 'training'), (('to', '</w>'), 'to</w>'), (('th', 'at</w>'), 'that</w>'), (('t', 'er'), 'ter'), (('r', 'o'), 'ro'), (('r', 'e</w>'), 're</w>'), (('p', 'us</w>'), 'pus</w>'), (('or', 'pus</w>'), 'orpus</w>'), (('on', '</w>'), 'on</w>'), (('mode', 'l</w>'), 'model</w>'), (('mo', 'st</w>'), 'most</w>'), (('l', 'y</w>'), 'ly</w>'), (('l', 'an'), 'lan')]\n",
      "Vocabulary: [',', '.', '2', '</w>', '?', 'B', 'C', 'I', 'T', 'W', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'â€”', 'â€™', 'e</w>', 't</w>', 'in', 'th', 's</w>', 'er', 'en', 'the</w>', 'to', 'or', 'd</w>', 'an', 'l</w>', 'er</w>', 'at</w>', 'mo', 'in</w>', 'yo', 'you', 'tok', 'token', 'tokeni', 'tokeniz', 'ra', 'a</w>', 'y</w>', 'tra', 'st</w>', 'r</w>', 'on', 'll</w>', 'de', '.</w>', ',</w>', 'your</w>', 'train', 'tokenizer</w>', 're', 'mode', 'ing', 'is</w>', 'f</w>', 'ed</w>', 'at', 'we</w>', 'us</w>', 'us', 'ua', 'uag', 'uage</w>', 'training', 'to</w>', 'that</w>', 'ter', 'ro', 're</w>', 'pus</w>', 'orpus</w>', 'on</w>', 'model</w>', 'most</w>', 'ly</w>', 'lan']\n",
      "Tokenized 'BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, letâ€™s say our corpus uses these five words:':\n",
      " B/P/E/ /training/ /s/t/a/r/t/s/ /b/y/ /c/o/m/p/u/t/ing/ /th/e/ /u/n/i/q/u/e/ /s/e/t/ /o/f/ /w/or/d/s/ /us/e/d/ /in/ /th/e/ /c/or/p/us/ /(/a/f/ter/ /th/e/ /n/or/m/a/l/i/z/at/i/on/ /an/d/ /p/re/-/tokeniz/at/i/on/ /s/t/e/p/s/ /a/re/ /c/o/m/p/l/e/t/e/d/)/,/ /th/en/ /b/u/i/l/d/ing/ /th/e/ /v/o/c/a/b/u/l/a/r/y/ /b/y/ /t/a/k/ing/ /a/l/l/ /th/e/ /s/y/m/b/o/l/s/ /us/e/d/ /to/ /w/r/i/t/e/ /th/o/s/e/ /w/or/d/s/./ /A/s/ /a/ /v/er/y/ /s/i/m/p/l/e/ /e/x/a/m/p/l/e/,/ /l/e/t/â€™/s/ /s/a/y/ /o/u/r/ /c/or/p/us/ /us/e/s/ /th/e/s/e/ /f/i/v/e/ /w/or/d/s/:/</w>\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def train_bpe(text, vocab_size):\n",
    "    # Preprocess the text into words and count frequencies\n",
    "    words = text.split()\n",
    "    word_freq = defaultdict(int)\n",
    "    for word in words:\n",
    "        chars = list(word) + ['</w>']\n",
    "        word_freq[tuple(chars)] += 1\n",
    "\n",
    "    # Initialize vocabulary with individual characters\n",
    "    vocab = set()\n",
    "    for word_seq in word_freq:\n",
    "        for char in word_seq:\n",
    "            vocab.add(char)\n",
    "    vocab = sorted(list(vocab))\n",
    "    merge_rules = []\n",
    "\n",
    "    while len(vocab) < vocab_size:\n",
    "        # Count frequency of each adjacent pair\n",
    "        pair_counts = defaultdict(int)\n",
    "        for seq, freq in word_freq.items():\n",
    "            for i in range(len(seq) - 1):\n",
    "                pair = (seq[i], seq[i+1])\n",
    "                pair_counts[pair] += freq\n",
    "\n",
    "        if not pair_counts:\n",
    "            break  # No more pairs to merge\n",
    "\n",
    "        # Select the most frequent pair\n",
    "        most_frequent_pair = max(pair_counts, key=lambda x: (pair_counts[x], x))\n",
    "\n",
    "        # Merge the most frequent pair\n",
    "        new_token = ''.join(most_frequent_pair)\n",
    "        merge_rules.append((most_frequent_pair, new_token))\n",
    "        vocab.append(new_token)\n",
    "\n",
    "        # Update word_freq with merged pairs\n",
    "        new_word_freq = defaultdict(int)\n",
    "        for seq, freq in word_freq.items():\n",
    "            new_seq = []\n",
    "            i = 0\n",
    "            while i < len(seq):\n",
    "                if i < len(seq)-1 and (seq[i], seq[i+1]) == most_frequent_pair:\n",
    "                    new_seq.append(new_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_seq.append(seq[i])\n",
    "                    i += 1\n",
    "            new_word_freq[tuple(new_seq)] += freq\n",
    "        word_freq = new_word_freq\n",
    "\n",
    "    return merge_rules, vocab\n",
    "\n",
    "def tokenize(word, merge_rules):\n",
    "    tokens = list(word) + ['</w>']\n",
    "    for pair, merged in merge_rules:\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens)-1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                new_tokens.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        tokens = new_tokens\n",
    "    return tokens\n",
    "\n",
    "# Training\n",
    "training_text = \"If a language model is not available in the language you are interested in, or if your corpus is very different from the one your language model was trained on, you will most likely want to retrain the model from scratch using a tokenizer adapted to your data. That will require training a new tokenizer on your dataset. But what exactly does that mean? When we first looked at tokenizers in Chapter 2, we saw that most Transformer models use a subword tokenization algorithm. To identify which subwords are of interest and occur most frequently in the corpus at hand, the tokenizer needs to take a hard look at all the texts in the corpus â€” a process we call training. The exact rules that govern this training depend on the type of tokenizer used, and weâ€™ll go over the three main algorithms later in this chapter.\"\n",
    "text = \"low lower lowest\"\n",
    "vocab_size = 100\n",
    "merge_rules, vocab = train_bpe(training_text, vocab_size)\n",
    "print(\"Merge Rules:\", merge_rules)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Tokenization\n",
    "test_word = \"BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, letâ€™s say our corpus uses these five words:\"\n",
    "tokenized = tokenize(test_word, merge_rules)\n",
    "print(f\"Tokenized '{test_word}':\\n\", \"/\".join(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'P',\n",
       " 'E',\n",
       " ' ',\n",
       " 't',\n",
       " 'ra',\n",
       " 'in',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'a',\n",
       " 'r',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'p',\n",
       " 'u',\n",
       " 't',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'i',\n",
       " 'q',\n",
       " 'u',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'w',\n",
       " 'or',\n",
       " 'd',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'or',\n",
       " 'p',\n",
       " 'u',\n",
       " 's',\n",
       " ' ',\n",
       " '(',\n",
       " 'a',\n",
       " 'f',\n",
       " 't',\n",
       " 'er',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'or',\n",
       " 'm',\n",
       " 'a',\n",
       " 'l',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'an',\n",
       " 'd',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " '-',\n",
       " 'tokeniz',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'p',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " ')',\n",
       " ',',\n",
       " ' ',\n",
       " 'th',\n",
       " 'en',\n",
       " ' ',\n",
       " 'b',\n",
       " 'u',\n",
       " 'i',\n",
       " 'l',\n",
       " 'd',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'v',\n",
       " 'o',\n",
       " 'c',\n",
       " 'a',\n",
       " 'b',\n",
       " 'u',\n",
       " 'l',\n",
       " 'a',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'a',\n",
       " 'k',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'y',\n",
       " 'm',\n",
       " 'b',\n",
       " 'o',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'w',\n",
       " 'r',\n",
       " 'i',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'th',\n",
       " 'o',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'or',\n",
       " 'd',\n",
       " 's',\n",
       " '.',\n",
       " ' ',\n",
       " 'A',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'v',\n",
       " 'er',\n",
       " 'y',\n",
       " ' ',\n",
       " 's',\n",
       " 'i',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'x',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ',',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " 'â€™',\n",
       " 's',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " ' ',\n",
       " 'c',\n",
       " 'or',\n",
       " 'p',\n",
       " 'u',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'or',\n",
       " 'd',\n",
       " 's',\n",
       " ':',\n",
       " '</w>']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word, merge_rules):\n",
    "    tokens = list(word) + ['</w>']\n",
    "    for pair, merged in merge_rules:\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens)-1 and tokens[i] == pair[0] and tokens[i+1] == pair[1]:\n",
    "                new_tokens.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        tokens = new_tokens\n",
    "    return tokens\n",
    "tokenized = tokenize(test_word, merge_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B',\n",
       " 'P',\n",
       " 'E',\n",
       " ' ',\n",
       " 't',\n",
       " 'ra',\n",
       " 'in',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'a',\n",
       " 'r',\n",
       " 't',\n",
       " 's',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'p',\n",
       " 'u',\n",
       " 't',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'u',\n",
       " 'n',\n",
       " 'i',\n",
       " 'q',\n",
       " 'u',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'e',\n",
       " 't',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'w',\n",
       " 'or',\n",
       " 'd',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'in',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'or',\n",
       " 'p',\n",
       " 'u',\n",
       " 's',\n",
       " ' ',\n",
       " '(',\n",
       " 'a',\n",
       " 'f',\n",
       " 't',\n",
       " 'er',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'n',\n",
       " 'or',\n",
       " 'm',\n",
       " 'a',\n",
       " 'l',\n",
       " 'i',\n",
       " 'z',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 'an',\n",
       " 'd',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " '-',\n",
       " 'tokeniz',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'e',\n",
       " 'p',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " 'e',\n",
       " 'd',\n",
       " ')',\n",
       " ',',\n",
       " ' ',\n",
       " 'th',\n",
       " 'en',\n",
       " ' ',\n",
       " 'b',\n",
       " 'u',\n",
       " 'i',\n",
       " 'l',\n",
       " 'd',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 'v',\n",
       " 'o',\n",
       " 'c',\n",
       " 'a',\n",
       " 'b',\n",
       " 'u',\n",
       " 'l',\n",
       " 'a',\n",
       " 'r',\n",
       " 'y',\n",
       " ' ',\n",
       " 'b',\n",
       " 'y',\n",
       " ' ',\n",
       " 't',\n",
       " 'a',\n",
       " 'k',\n",
       " 'in',\n",
       " 'g',\n",
       " ' ',\n",
       " 'a',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'y',\n",
       " 'm',\n",
       " 'b',\n",
       " 'o',\n",
       " 'l',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 'd',\n",
       " ' ',\n",
       " 'to',\n",
       " ' ',\n",
       " 'w',\n",
       " 'r',\n",
       " 'i',\n",
       " 't',\n",
       " 'e',\n",
       " ' ',\n",
       " 'th',\n",
       " 'o',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'or',\n",
       " 'd',\n",
       " 's',\n",
       " '.',\n",
       " ' ',\n",
       " 'A',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'v',\n",
       " 'er',\n",
       " 'y',\n",
       " ' ',\n",
       " 's',\n",
       " 'i',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'e',\n",
       " 'x',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ',',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 't',\n",
       " 'â€™',\n",
       " 's',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'y',\n",
       " ' ',\n",
       " 'o',\n",
       " 'u',\n",
       " 'r',\n",
       " ' ',\n",
       " 'c',\n",
       " 'or',\n",
       " 'p',\n",
       " 'u',\n",
       " 's',\n",
       " ' ',\n",
       " 'u',\n",
       " 's',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'th',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'v',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'or',\n",
       " 'd',\n",
       " 's',\n",
       " ':',\n",
       " '</w>']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
