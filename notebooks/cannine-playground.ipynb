{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['this', 'is', 'a', 'simple', 'example', 'of', 'the', 'hashing', 'trick', 'in', 'action']\n",
      "Feature vector: [2. 1. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 1. 0. 1. 0. 2.]\n",
      "\n",
      "Tokens: ['another', 'example', 'of', 'the', 'hashing', 'trick']\n",
      "Feature vector: [2. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.]\n",
      "\n",
      "Positions where collisions might occur:\n",
      "Position 0: 2.0 vs 2.0\n",
      "Position 1: 1.0 vs 1.0\n",
      "Position 15: 1.0 vs 1.0\n",
      "Position 19: 2.0 vs 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hashlib import md5\n",
    "\n",
    "def hash_trick(tokens, n_dimensions=1000):\n",
    "    \"\"\"\n",
    "    Implements the hashing trick for text feature representation.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of strings (words or n-grams)\n",
    "        n_dimensions: Size of the feature vector\n",
    "    \n",
    "    Returns:\n",
    "        A feature vector of size n_dimensions\n",
    "    \"\"\"\n",
    "    # Initialize the feature vector with zeros\n",
    "    feature_vector = np.zeros(n_dimensions)\n",
    "    \n",
    "    # For each token in the input\n",
    "    for token in tokens:\n",
    "        # Hash the token to get an integer\n",
    "        hash_value = int(md5(token.encode('utf-8')).hexdigest(), 16)\n",
    "        \n",
    "        # Map the hash value to a position in the feature vector\n",
    "        position = hash_value % n_dimensions\n",
    "        \n",
    "        # Increment the count at that position\n",
    "        feature_vector[position] += 1\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# Example usage\n",
    "text = \"this is a simple example of the hashing trick in action\"\n",
    "tokens = text.lower().split()\n",
    "\n",
    "# Create feature vector with 20 dimensions\n",
    "feature_vector = hash_trick(tokens, n_dimensions=20)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Feature vector:\", feature_vector)\n",
    "\n",
    "# Let's see what happens with two different texts that share some words\n",
    "text2 = \"another example of the hashing trick\"\n",
    "tokens2 = text2.lower().split()\n",
    "feature_vector2 = hash_trick(tokens2, n_dimensions=20)\n",
    "\n",
    "print(\"\\nTokens:\", tokens2)\n",
    "print(\"Feature vector:\", feature_vector2)\n",
    "\n",
    "# Some common words will map to the same position in both vectors\n",
    "print(\"\\nPositions where collisions might occur:\")\n",
    "for i in range(len(feature_vector)):\n",
    "    if feature_vector[i] > 0 and feature_vector2[i] > 0:\n",
    "        print(f\"Position {i}: {feature_vector[i]} vs {feature_vector2[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_trick(['hello', 'world'], n_dimensions=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 50])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HashEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_hashes, pool_size):\n",
    "        super(HashEmbedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_hashes = num_hashes\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        # Shared pool of embedding vectors\n",
    "        self.embedding_pool = nn.Embedding(pool_size, embedding_dim)\n",
    "\n",
    "        # Importance parameters for each word\n",
    "        self.importance_weights = nn.Embedding(vocab_size, num_hashes)\n",
    "\n",
    "        # Hash functions (random but fixed). \n",
    "        # technically, this is really big with pool_size * vocab_size but it's just for the sake of easy implementation.\n",
    "        self.hash_functions = [\n",
    "            torch.randint(0, pool_size, (vocab_size,)) for _ in range(num_hashes)]\n",
    "\n",
    "    def forward(self, word_ids):\n",
    "        # Get importance weights for the input words\n",
    "        weights = self.importance_weights(word_ids)  # Shape: (batch_size, num_hashes)\n",
    "\n",
    "        # Get component vectors using hash functions\n",
    "        component_vectors = []\n",
    "        for i in range(self.num_hashes):\n",
    "            # Apply hash function to get indices into the shared pool\n",
    "            indices = self.hash_functions[i][word_ids]  # Shape: (batch_size,)\n",
    "            # Lookup vectors from the shared pool\n",
    "            vectors = self.embedding_pool(indices)  # Shape: (batch_size, embedding_dim)\n",
    "            component_vectors.append(vectors)\n",
    "\n",
    "        # Stack component vectors and compute weighted sum\n",
    "        component_vectors = torch.stack(component_vectors, dim=1)  # Shape: (batch_size, num_hashes, embedding_dim)\n",
    "        weights = weights.unsqueeze(-1)  # Shape: (batch_size, num_hashes, 1)\n",
    "        final_embeddings = (weights * component_vectors).sum(dim=1)  # Shape: (batch_size, embedding_dim)\n",
    "\n",
    "        return final_embeddings\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 10000  # Number of unique words\n",
    "embedding_dim = 50   # Dimension of each embedding vector\n",
    "num_hashes = 2       # Number of hash functions\n",
    "pool_size = 1000     # Size of the shared embedding pool\n",
    "\n",
    "# Create hash embedding layer\n",
    "hash_embedding = HashEmbedding(vocab_size, embedding_dim, num_hashes, pool_size)\n",
    "\n",
    "# Input: batch of word IDs (e.g., [3, 7, 2])\n",
    "word_ids = torch.tensor([3, 7, 2])\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = hash_embedding(word_ids)\n",
    "print(embeddings.shape)  # Output: (3, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_embedding.hash_functions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0106e-01, -9.4563e-01,  1.1096e+00,  1.2103e+00,  4.2908e-01,\n",
       "         -1.4054e-01, -2.9341e-01, -1.4251e+00, -1.0027e+00,  6.0199e-01,\n",
       "          7.8202e-01,  8.9964e-01, -6.9072e-01,  8.4406e-01,  3.8960e-02,\n",
       "          5.5408e-01, -2.1158e-01, -1.0466e+00, -7.9575e-01, -7.2971e-01,\n",
       "         -2.2697e-01, -5.1622e-01, -3.2555e-01, -1.3523e-01, -4.0508e-01,\n",
       "          7.6875e-01,  8.1700e-01,  8.0329e-01,  8.3822e-01, -9.8230e-01,\n",
       "         -7.5856e-02,  7.7942e-01, -4.5099e-01,  6.5127e-01, -5.1193e-02,\n",
       "          2.0867e-01, -5.6362e-01,  2.1198e-01,  1.9371e-01, -1.4677e+00,\n",
       "         -3.5016e-01,  2.7238e-01,  9.2124e-01,  3.3926e-01, -8.9065e-01,\n",
       "          9.0552e-02,  5.5217e-01,  1.9165e+00,  4.4012e-01, -7.9510e-03],\n",
       "        [-9.9130e-01,  2.1554e+00,  9.0176e-01,  2.4565e+00,  2.6315e+00,\n",
       "         -3.7404e+00,  3.5755e-01,  1.0505e-01, -5.2969e-01,  6.0918e-01,\n",
       "         -6.5580e-01, -1.4672e+00,  5.2210e-01,  6.6820e-01, -2.7233e+00,\n",
       "          3.2347e+00,  2.5359e+00, -2.4846e+00, -2.4237e-01, -1.9045e+00,\n",
       "         -1.4426e-01, -1.9752e+00, -6.2770e-01,  9.8232e-01, -3.8458e+00,\n",
       "         -2.4210e+00, -7.9258e-01, -6.5587e-01, -4.7937e+00, -1.5619e+00,\n",
       "          8.6215e-01, -7.1980e-01, -2.6745e+00, -2.7711e+00, -1.1663e+00,\n",
       "         -1.1687e+00, -2.8579e+00, -4.0340e+00,  1.6023e+00, -1.7982e-01,\n",
       "         -6.1037e-01,  2.4586e+00,  2.5710e+00, -6.0016e-01,  6.4778e-01,\n",
       "         -4.6725e+00,  1.4134e+00,  1.9265e+00, -2.3702e+00,  1.1733e+00],\n",
       "        [-2.0556e-03,  6.1466e-01, -7.4571e-01,  1.1090e-01, -7.8798e-01,\n",
       "         -2.1117e+00,  4.5943e-01,  2.9587e-01,  1.1032e-01, -8.6485e-01,\n",
       "          1.9951e-01,  4.9296e-01,  5.0607e-01,  1.8396e-01, -7.7639e-01,\n",
       "         -9.7333e-02, -3.7886e-02,  1.2489e-01,  8.9786e-01,  5.0119e-01,\n",
       "         -4.3642e-01,  8.0607e-01, -2.2772e+00, -1.3444e+00, -4.9870e-01,\n",
       "          5.4102e-01, -5.5921e-01, -6.8093e-01,  5.0508e-02, -1.5614e+00,\n",
       "          2.0979e-01,  3.3230e-01,  1.9845e+00, -4.8846e-01, -4.8782e-01,\n",
       "          1.5416e+00, -1.2439e+00,  1.9818e+00,  6.6421e-01,  3.7412e-01,\n",
       "         -1.7169e+00, -7.2304e-01,  3.1324e-01, -3.1548e-01, -2.5618e-01,\n",
       "          8.9648e-01, -1.2083e+00, -8.3230e-02, -5.1224e-01,  3.4270e-01]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = hash_embedding(word_ids)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
